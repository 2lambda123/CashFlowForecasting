{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Models on Monthly aggregated Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# libraries:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6081, 5)\n",
      "(6081, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>account_id</th>\n",
       "      <th>partition_ledger_year_month</th>\n",
       "      <th>amount_usd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>550385</td>\n",
       "      <td>201501</td>\n",
       "      <td>-390217.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>550385</td>\n",
       "      <td>201502</td>\n",
       "      <td>230944.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>550385</td>\n",
       "      <td>201503</td>\n",
       "      <td>367259.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>550385</td>\n",
       "      <td>201504</td>\n",
       "      <td>567962.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>550385</td>\n",
       "      <td>201505</td>\n",
       "      <td>753175.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   account_id  partition_ledger_year_month  amount_usd\n",
       "0      550385                       201501  -390217.24\n",
       "1      550385                       201502   230944.09\n",
       "2      550385                       201503   367259.69\n",
       "3      550385                       201504   567962.85\n",
       "4      550385                       201505   753175.60"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We tried two ways after loading the data, 1. Normal string value for \"partition_ledger_year_month\" feature\n",
    "# and also datetime value for \"partition_ledger_year_month\" feature. But, the string value proved to give \n",
    "# better results:\n",
    "\n",
    "model_df = pd.read_csv('../data/monthly_data_training.csv')\n",
    "print(model_df.shape)\n",
    "\n",
    "# Dropping two columns \"GL_account\" and \"GL_account_description\" as they are redundant and not required\n",
    "# for the model to learn the amound_usd series:\n",
    "\n",
    "model_df = model_df.drop([\"GL_account\", \"GL_account_description\"], axis = 1)\n",
    "print(model_df.shape)\n",
    "# Tried datatime approach which is commented as of now:\n",
    "#model_df.date_general_ledger = pd.to_datetime(model_df.date_general_ledger)\n",
    "\n",
    "# Printing top 5 entries in the final dataframe:\n",
    "model_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>account_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>550385</th>\n",
       "      <td>54.0</td>\n",
       "      <td>-7.019257e+06</td>\n",
       "      <td>1.161007e+07</td>\n",
       "      <td>-5.506018e+07</td>\n",
       "      <td>-7.659706e+06</td>\n",
       "      <td>-4.302384e+06</td>\n",
       "      <td>-4.127188e+05</td>\n",
       "      <td>5.159147e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550406</th>\n",
       "      <td>54.0</td>\n",
       "      <td>-7.849639e+07</td>\n",
       "      <td>9.242308e+07</td>\n",
       "      <td>-4.380654e+08</td>\n",
       "      <td>-1.425767e+08</td>\n",
       "      <td>-7.239205e+07</td>\n",
       "      <td>-7.645546e+05</td>\n",
       "      <td>8.774756e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661926</th>\n",
       "      <td>54.0</td>\n",
       "      <td>-3.624034e+06</td>\n",
       "      <td>3.491522e+07</td>\n",
       "      <td>-1.600823e+08</td>\n",
       "      <td>-2.238820e+07</td>\n",
       "      <td>4.256758e+06</td>\n",
       "      <td>1.637850e+07</td>\n",
       "      <td>4.627042e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693819</th>\n",
       "      <td>50.0</td>\n",
       "      <td>2.602236e+02</td>\n",
       "      <td>2.351533e+03</td>\n",
       "      <td>-1.374500e+03</td>\n",
       "      <td>-2.962825e+02</td>\n",
       "      <td>-8.944000e+01</td>\n",
       "      <td>2.277825e+02</td>\n",
       "      <td>1.631881e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751280</th>\n",
       "      <td>54.0</td>\n",
       "      <td>-2.156726e+07</td>\n",
       "      <td>1.055403e+08</td>\n",
       "      <td>-2.318708e+08</td>\n",
       "      <td>-8.325770e+07</td>\n",
       "      <td>-1.566982e+07</td>\n",
       "      <td>2.441177e+07</td>\n",
       "      <td>5.277015e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3169158</th>\n",
       "      <td>16.0</td>\n",
       "      <td>-8.641250e+00</td>\n",
       "      <td>3.456500e+01</td>\n",
       "      <td>-1.382600e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3274874</th>\n",
       "      <td>10.0</td>\n",
       "      <td>-5.062899e+04</td>\n",
       "      <td>1.657069e+05</td>\n",
       "      <td>-5.210531e+05</td>\n",
       "      <td>-5.374150e+03</td>\n",
       "      <td>-1.022400e+02</td>\n",
       "      <td>2.792755e+03</td>\n",
       "      <td>2.290221e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3283940</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2.905001e+05</td>\n",
       "      <td>1.342318e+06</td>\n",
       "      <td>-8.994371e+05</td>\n",
       "      <td>-8.186797e+05</td>\n",
       "      <td>-1.206540e+05</td>\n",
       "      <td>1.135920e+06</td>\n",
       "      <td>2.419111e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3291608</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2.819717e+03</td>\n",
       "      <td>4.820303e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.749835e+03</td>\n",
       "      <td>1.023835e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3301816</th>\n",
       "      <td>4.0</td>\n",
       "      <td>8.559565e+04</td>\n",
       "      <td>2.846643e+05</td>\n",
       "      <td>-1.494705e+05</td>\n",
       "      <td>-4.347781e+04</td>\n",
       "      <td>-4.073460e+03</td>\n",
       "      <td>1.250000e+05</td>\n",
       "      <td>5.000000e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>129 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            count          mean           std           min           25%  \\\n",
       "account_id                                                                  \n",
       "550385       54.0 -7.019257e+06  1.161007e+07 -5.506018e+07 -7.659706e+06   \n",
       "550406       54.0 -7.849639e+07  9.242308e+07 -4.380654e+08 -1.425767e+08   \n",
       "661926       54.0 -3.624034e+06  3.491522e+07 -1.600823e+08 -2.238820e+07   \n",
       "693819       50.0  2.602236e+02  2.351533e+03 -1.374500e+03 -2.962825e+02   \n",
       "751280       54.0 -2.156726e+07  1.055403e+08 -2.318708e+08 -8.325770e+07   \n",
       "...           ...           ...           ...           ...           ...   \n",
       "3169158      16.0 -8.641250e+00  3.456500e+01 -1.382600e+02  0.000000e+00   \n",
       "3274874      10.0 -5.062899e+04  1.657069e+05 -5.210531e+05 -5.374150e+03   \n",
       "3283940       7.0  2.905001e+05  1.342318e+06 -8.994371e+05 -8.186797e+05   \n",
       "3291608       7.0  2.819717e+03  4.820303e+03  0.000000e+00  0.000000e+00   \n",
       "3301816       4.0  8.559565e+04  2.846643e+05 -1.494705e+05 -4.347781e+04   \n",
       "\n",
       "                     50%           75%           max  \n",
       "account_id                                            \n",
       "550385     -4.302384e+06 -4.127188e+05  5.159147e+06  \n",
       "550406     -7.239205e+07 -7.645546e+05  8.774756e+07  \n",
       "661926      4.256758e+06  1.637850e+07  4.627042e+07  \n",
       "693819     -8.944000e+01  2.277825e+02  1.631881e+04  \n",
       "751280     -1.566982e+07  2.441177e+07  5.277015e+08  \n",
       "...                  ...           ...           ...  \n",
       "3169158     0.000000e+00  0.000000e+00  0.000000e+00  \n",
       "3274874    -1.022400e+02  2.792755e+03  2.290221e+04  \n",
       "3283940    -1.206540e+05  1.135920e+06  2.419111e+06  \n",
       "3291608     0.000000e+00  4.749835e+03  1.023835e+04  \n",
       "3301816    -4.073460e+03  1.250000e+05  5.000000e+05  \n",
       "\n",
       "[129 rows x 8 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is to decribe the data grouping them by account_id, which shows the statistic values of the feature\n",
    "# \"amount_usd\":\n",
    "\n",
    "model_df.groupby('account_id')['amount_usd'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1: using string for \"partition_ledger_year_month\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   partition_ledger_year_month\n",
      "0                       201907\n",
      "1                       201907\n",
      "2                       201907\n",
      "3                       201907\n",
      "4                       201907\n",
      "   account_id    amount_usd\n",
      "0      550385 -7.620617e+06\n",
      "1      550406 -1.619195e+08\n",
      "2      661926  2.715227e+06\n",
      "3      693819 -5.260680e+02\n",
      "4      751280 -1.033526e+08\n"
     ]
    }
   ],
   "source": [
    "# Loading the submission file with our ouput ledger year month to be predicted for. (i.e. 201907)\n",
    "\n",
    "test_df = pd.read_csv(\"submissionKey_2019_7_testfile.csv\")\n",
    "\n",
    "# Dropping columns [\"account_id\",\"amount_usd\"] as we only require \"partition_ledger_year_month\" for X_test:\n",
    "X_test = test_df.drop([\"account_id\",\"amount_usd\"], axis=1)\n",
    "print(X_test.head())\n",
    "\n",
    "# Function for Linear Regression model with returning predicted values for \"201907\" partition_ledger_year_month\n",
    "# This function is called once for each account id as it is fired on groupby \"account_id\"\n",
    "def model(df):\n",
    "    X = df[['partition_ledger_year_month']].values\n",
    "    y = df[['amount_usd']].values\n",
    "    return np.squeeze(LinearRegression().fit(X, y).predict(X_test))[0]\n",
    "\n",
    "\n",
    "# Function to apply model function on each group:\n",
    "def group_predictions(df):\n",
    "    return df.groupby('account_id').apply(model)\n",
    "\n",
    "# Main driver code\n",
    "submission_df = group_predictions(model_df).reset_index(name='amount_usd')\n",
    "print(submission_df.head())\n",
    "\n",
    "# Saving results to the submission csv:\n",
    "submission_df.to_csv(\"submissionKey_2019_7_LinearRegression_StringDateApproach.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OR Approach 2: using datetime for \"partition_ledger_year_month\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   partition_ledger_year_month\n",
      "0                       201907\n",
      "1                       201907\n",
      "2                       201907\n",
      "3                       201907\n",
      "4                       201907\n",
      "2015-03-01 00:00:00\n",
      "   account_id    amount_usd\n",
      "0      550385  2.018596e+06\n",
      "1      550406  1.800846e+09\n",
      "2      661926 -1.995521e+08\n",
      "3      693819  2.085647e+04\n",
      "4      751280  1.687867e+09\n"
     ]
    }
   ],
   "source": [
    "# Loading the submission file with our ouput ledger year month to be predicted for. (i.e. 201907)\n",
    "test_df = pd.read_csv(\"submissionKey_2019_7_testfile.csv\")\n",
    "\n",
    "# Dropping columns [\"account_id\",\"amount_usd\"] as we only require \"partition_ledger_year_month\" for X_test:\n",
    "X_test = test_df.drop([\"account_id\",\"amount_usd\"], axis=1)\n",
    "print(X_test.head())\n",
    "\n",
    "# converting \"partition_ledger_year_month\" to datetime:\n",
    "model_df.partition_ledger_year_month = pd.to_datetime(model_df.partition_ledger_year_month, format=\"%Y%m\")\n",
    "print(model_df.partition_ledger_year_month[2])\n",
    "\n",
    "# Function for Linear Regression model with returning predicted values for \"201907\" partition_ledger_year_month\n",
    "# This function is called once for each account id as it is fired on groupby \"account_id\"\n",
    "def model(data, xvars, yvar):\n",
    "    X = data[xvars]\n",
    "    Y = data[yvar]\n",
    "    model = LinearRegression()\n",
    "    mod = model\n",
    "    mod.fit(X, Y)\n",
    "    predictions = mod.predict(X_test)\n",
    "    return predictions[0][0]\n",
    "\n",
    "# Function to apply model function on each group and passing columns \n",
    "# ['partition_ledger_year_month'], ['amount_usd'] as X and Y to fit the model:\n",
    "submission_df = model_df.groupby('account_id').apply(model, ['partition_ledger_year_month'], ['amount_usd']).reset_index(name='amount_usd')\n",
    "print(submission_df.head())\n",
    "\n",
    "# Saving results to the submission csv:\n",
    "submission_df.to_csv(\"submissionKey_2019_7_LinearRegression_DatetimeApproach.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion: \n",
    "### After submission of both the appraoches for Linear Regression, we found that Approach 1 using string value for \"partition_ledger_year_month\" is better, so for further approaches we would be only using one approach that is Approach 1 string value.\n",
    "# .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Libraries:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6081, 3)\n"
     ]
    }
   ],
   "source": [
    "# loading the training data:\n",
    "model_df = pd.read_csv('../data/monthly_data_training.csv')\n",
    "\n",
    "# Dropping two columns \"GL_account\" and \"GL_account_description\" as they are redundant and not required\n",
    "# for the model to learn the amound_usd series:\n",
    "model_df = model_df.drop([\"GL_account\", \"GL_account_description\"], axis = 1)\n",
    "print(model_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF Model 1: n_estimators=100, max_depth=4  (n_estimators means no. of trees and max depth of each tree )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   account_id    amount_usd\n",
      "0      550385 -6.065207e+06\n",
      "1      550406 -1.575830e+08\n",
      "2      661926  9.447851e+03\n",
      "3      693819 -2.642580e+01\n",
      "4      751280 -5.576986e+07\n"
     ]
    }
   ],
   "source": [
    "# Loading the submission file with our ouput ledger year month to be predicted for. (i.e. 201907)\n",
    "test_df = pd.read_csv(\"submissionKey_2019_7_testfile.csv\")\n",
    "\n",
    "# Dropping columns [\"account_id\",\"amount_usd\"] as we only require \"partition_ledger_year_month\" for X_test:\n",
    "X_test = test_df.drop([\"account_id\", \"amount_usd\"], axis=1)\n",
    "\n",
    "# Function for Random Forest model with returning predicted values for \"201907\" partition_ledger_year_month\n",
    "# This function is called once for each account id as it is fired on groupby \"account_id\"\n",
    "# Function to apply model function on each group and passing columns \n",
    "# ['partition_ledger_year_month'], ['amount_usd'] as X and Y to fit the model:\n",
    "def model(df, model_object):\n",
    "    X = df[['partition_ledger_year_month']].values\n",
    "    y = df[['amount_usd']].values\n",
    "    return model_object.fit(X, y.ravel()).predict(X_test)[0]\n",
    "\n",
    "# Function to apply model function on each group:\n",
    "def group_predictions(df):\n",
    "    model_object = RandomForestRegressor(n_estimators=100, max_depth=4)\n",
    "    return df.groupby('account_id').apply(model, model_object)\n",
    "\n",
    "# Main driver code\n",
    "submission_df = group_predictions(model_df).reset_index(name='amount_usd')\n",
    "print(submission_df.head())\n",
    "\n",
    "# Saving results to the submission csv:\n",
    "submission_df.to_csv(\"submissionKey_2019_7_RandomForest_n_estimators_100_maxdepth_4.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF Model 2: n_estimators=300, max_depth=30 (n_estimators means no. of trees and max depth of each tree )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   account_id    amount_usd\n",
      "0      550385 -6.154337e+06\n",
      "1      550406 -1.459862e+08\n",
      "2      661926 -1.767756e+06\n",
      "3      693819  2.346733e+01\n",
      "4      751280 -4.916446e+07\n"
     ]
    }
   ],
   "source": [
    "# Loading the submission file with our ouput ledger year month to be predicted for. (i.e. 201907)\n",
    "test_df = pd.read_csv(\"submissionKey_2019_7_testfile.csv\")\n",
    "\n",
    "# Dropping columns [\"account_id\",\"amount_usd\"] as we only require \"partition_ledger_year_month\" for X_test:\n",
    "X_test = test_df.drop([\"account_id\", \"amount_usd\"], axis=1)\n",
    "\n",
    "# Function for Random Forest model with returning predicted values for \"201907\" partition_ledger_year_month\n",
    "# This function is called once for each account id as it is fired on groupby \"account_id\"\n",
    "# Function to apply model function on each group and passing columns \n",
    "# ['partition_ledger_year_month'], ['amount_usd'] as X and Y to fit the model:\n",
    "def model(df, model_object):\n",
    "    X = df[['partition_ledger_year_month']].values\n",
    "    y = df[['amount_usd']].values\n",
    "    return model_object.fit(X, y.ravel()).predict(X_test)[0]\n",
    "\n",
    "# Function to apply model function on each group:\n",
    "def group_predictions(df):\n",
    "    model_object = RandomForestRegressor(n_estimators=300, max_depth=30)\n",
    "    return df.groupby('account_id').apply(model, model_object)\n",
    "\n",
    "# Main driver code\n",
    "submission_df = group_predictions(model_df).reset_index(name='amount_usd')\n",
    "print(submission_df.head())\n",
    "\n",
    "# Saving results to the submission csv:\n",
    "submission_df.to_csv(\"submissionKey_2019_7_RandomForest_n_estimators_300_maxdepth_30.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Libraries:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost.sklearn import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6081, 3)\n"
     ]
    }
   ],
   "source": [
    "# loading the training data:\n",
    "model_df = pd.read_csv('../data/monthly_data_training.csv')\n",
    "\n",
    "# Dropping two columns \"GL_account\" and \"GL_account_description\" as they are redundant and not required\n",
    "# for the model to learn the amound_usd series:\n",
    "model_df = model_df.drop([\"GL_account\", \"GL_account_description\"], axis = 1)\n",
    "print(model_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model 1: n_estimators=100, learning_rate=0.2 (n_estimators means no. of trees and learning rate implies the rate at which the new values needs to be updated to the next training epoch/iteration )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   account_id    amount_usd\n",
      "0      550385 -7.613681e+06\n",
      "1      550406 -1.489382e+08\n",
      "2      661926 -8.880184e+06\n",
      "3      693819  1.176825e+02\n",
      "4      751280 -2.496816e+07\n"
     ]
    }
   ],
   "source": [
    "# Loading the submission file with our ouput ledger year month to be predicted for. (i.e. 201907)\n",
    "\n",
    "test_df = pd.read_csv(\"submissionKey_2019_7_testfile.csv\")\n",
    "\n",
    "# Dropping columns [\"account_id\",\"amount_usd\"] as we only require \"partition_ledger_year_month\" for X_test:\n",
    "X_test = test_df.drop([\"account_id\", \"amount_usd\"], axis=1)\n",
    "\n",
    "# Function for XG Boost model with returning predicted values for \"201907\" partition_ledger_year_month\n",
    "# This function is called once for each account id as it is fired on groupby \"account_id\"\n",
    "def model(data, xvars, yvar):\n",
    "    X = data[xvars]\n",
    "    Y = data[yvar]\n",
    "    model = XGBRegressor(n_estimators=100, learning_rate=0.2, objective='reg:squarederror')\n",
    "    mod = model\n",
    "    mod.fit(X, Y)\n",
    "    predictions = mod.predict(X_test)\n",
    "    return predictions[0]\n",
    "\n",
    "# Main driver code\n",
    "# Function to apply model function on each group and passing columns \n",
    "# ['partition_ledger_year_month'], ['amount_usd'] as X and Y to fit the model:\n",
    "submission_df = model_df.groupby('account_id').apply(model, ['partition_ledger_year_month'], ['amount_usd']).reset_index(name='amount_usd')\n",
    "print(submission_df.head())\n",
    "\n",
    "# Saving results to the submission csv:\n",
    "submission_df.to_csv(\"submissionKey_2019_7_XGBoost_n_estimators_100_lr_0.2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model 2: n_estimators=1000, learning_rate=0.3 (n_estimators means no. of trees and learning rate implies the rate at which the new values needs to be updated to the next training epoch/iteration )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   account_id    amount_usd\n",
      "0      550385 -7.657766e+06\n",
      "1      550406 -1.499432e+08\n",
      "2      661926 -8.996900e+06\n",
      "3      693819  1.229789e+02\n",
      "4      751280 -2.468563e+07\n"
     ]
    }
   ],
   "source": [
    "# Loading the submission file with our ouput ledger year month to be predicted for. (i.e. 201907)\n",
    "\n",
    "test_df = pd.read_csv(\"submissionKey_2019_7_testfile.csv\")\n",
    "\n",
    "# Dropping columns [\"account_id\",\"amount_usd\"] as we only require \"partition_ledger_year_month\" for X_test:\n",
    "X_test = test_df.drop([\"account_id\", \"amount_usd\"], axis=1)\n",
    "\n",
    "# Function for XG Boost model with returning predicted values for \"201907\" partition_ledger_year_month\n",
    "# This function is called once for each account id as it is fired on groupby \"account_id\"\n",
    "def model(data, xvars, yvar):\n",
    "    X = data[xvars]\n",
    "    Y = data[yvar]\n",
    "    model = XGBRegressor(n_estimators=1000, learning_rate=0.3, objective='reg:squarederror')\n",
    "    mod = model\n",
    "    mod.fit(X, Y)\n",
    "    predictions = mod.predict(X_test)\n",
    "    return predictions[0]\n",
    "\n",
    "# Main driver code\n",
    "# Function to apply model function on each group and passing columns \n",
    "# ['partition_ledger_year_month'], ['amount_usd'] as X and Y to fit the model:\n",
    "submission_df = model_df.groupby('account_id').apply(model, ['partition_ledger_year_month'], ['amount_usd']).reset_index(name='amount_usd')\n",
    "print(submission_df.head())\n",
    "\n",
    "# Saving results to the submission csv:\n",
    "submission_df.to_csv(\"submissionKey_2019_7_XGBoost_n_estimators_1000_lr_0.3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  4. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# libraries:\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam \n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import LSTM\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6081, 3)\n"
     ]
    }
   ],
   "source": [
    "# loading the training data:\n",
    "model_df = pd.read_csv('../data/monthly_data_training.csv')\n",
    "\n",
    "# Dropping two columns \"GL_account\" and \"GL_account_description\" as they are redundant and not required\n",
    "# for the model to learn the amound_usd series:\n",
    "model_df = model_df.drop([\"GL_account\", \"GL_account_description\"], axis = 1)\n",
    "print(model_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6081\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 1s 17ms/step - loss: 0.4655\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 1s 9ms/step - loss: 0.4692\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 1s 11ms/step - loss: 0.4408\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.5752\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 0.4465\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 1s 10ms/step - loss: 0.5399\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 1s 10ms/step - loss: 0.6094\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 1s 10ms/step - loss: 0.4400\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 1s 9ms/step - loss: 0.5757\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 0.4829\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.4358\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 0.5121\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.4329\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 1s 9ms/step - loss: 0.4311\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 0.4490\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 1s 10ms/step - loss: 0.6022\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 1s 11ms/step - loss: 0.7869\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 1s 11ms/step - loss: 0.4751\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 1s 10ms/step - loss: 0.4989\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.4495\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.7990\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.5011\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.4524\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.4950\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.4744\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.4364\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.4905\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.4676\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.4760\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 1s 11ms/step - loss: 0.6162\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 1s 11ms/step - loss: 0.4247\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 0.4765\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 1s 10ms/step - loss: 0.5235\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 1s 10ms/step - loss: 0.4922\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 0.5263\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 0.5081\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 0.4723\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 1s 11ms/step - loss: 0.4518\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 1.0384\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 0.4497\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 1s 10ms/step - loss: 0.4793\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "51/51 [==============================] - 0s 9ms/step - loss: 0.4996\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 0.5324\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 1s 12ms/step - loss: 0.5347\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.4472\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 0.4954\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.4543\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 12ms/step - loss: 0.4164\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.5327\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 0.7235\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 0.7812\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.4842\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.9230\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.6059\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.4902\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.4353\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.4111\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.4311\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.4349\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.4489\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.4529\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.4451\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.4848\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.3196\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.4794\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.3935\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.5281\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.4509\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 0.4601\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.4717\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.5656\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.4976\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.4708\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.4395\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.4353\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.4413\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.2744\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.3886\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "7/7 [==============================] - 0s 51ms/step - loss: 1.0223\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "20/20 [==============================] - 0s 19ms/step - loss: 0.3903\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.5139\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 0.4888\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 0.4910\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.5098\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.6124\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 1s 10ms/step - loss: 0.5045\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 0.4563\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "20/20 [==============================] - 0s 19ms/step - loss: 0.3717\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.4591\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 0.6190\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 0.4589\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.5090\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.5026\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.4576\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 0.4629\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.3892\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.4725\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.4687\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.5727\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.3344\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.5559\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "31/31 [==============================] - 0s 14ms/step - loss: 0.3521\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "28/28 [==============================] - 0s 16ms/step - loss: 0.3697\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.4718\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 0.4798\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.5236\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.4756\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.4743\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.4957\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "39/39 [==============================] - 1s 14ms/step - loss: 0.4432\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "39/39 [==============================] - 0s 12ms/step - loss: 0.4428\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "39/39 [==============================] - 0s 11ms/step - loss: 0.4508\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "39/39 [==============================] - 0s 12ms/step - loss: 0.4646\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "39/39 [==============================] - 0s 11ms/step - loss: 0.4425\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "36/36 [==============================] - 0s 12ms/step - loss: 0.4204\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 1s 15ms/step - loss: 0.5017\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "13/13 [==============================] - 0s 32ms/step - loss: 0.8453\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "25/25 [==============================] - 1s 22ms/step - loss: 0.4970\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "25/25 [==============================] - 0s 16ms/step - loss: 0.6123\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "7/7 [==============================] - 0s 69ms/step - loss: 0.9113\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.4423\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 1.0297\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.9449\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.8662\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "10/10 [==============================] - 0s 48ms/step - loss: 0.9208\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "7/7 [==============================] - 0s 55ms/step - loss: 0.8877\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "7/7 [==============================] - 1s 82ms/step - loss: 0.9161\n",
      "(129, 1)\n",
      "Epoch 1/1\n",
      "4/4 [==============================] - 0s 107ms/step - loss: 0.6043\n",
      "(129, 1)\n",
      "   account_id     amount_usd\n",
      "0      550385  201667.090388\n",
      "1      550406  201691.416896\n",
      "2      661926  201694.402372\n",
      "3      693819  201658.470490\n",
      "4      751280  201691.453421\n"
     ]
    }
   ],
   "source": [
    "# Scaling, UnScaling and prediction functions\n",
    "train_rows = int(model_df.shape[0])\n",
    "print(train_rows)\n",
    "\n",
    "def scale_data(train_set, test_set):\n",
    "    #apply Min Max Scaler\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler = scaler.fit(train_set)\n",
    "    \n",
    "    # reshape training set\n",
    "    train_set = train_set.reshape(train_set.shape[0], train_set.shape[1])\n",
    "    train_set_scaled = scaler.transform(train_set)\n",
    "    \n",
    "    # reshape test set\n",
    "    test_set = test_set.reshape(test_set.shape[0], test_set.shape[1])\n",
    "    test_set_scaled = scaler.transform(test_set)\n",
    "    \n",
    "    X_train, y_train = train_set_scaled[:, 1:], train_set_scaled[:, 0:1].ravel()\n",
    "    X_test, y_test = test_set_scaled[:, 1:], test_set_scaled[:, 0:1].ravel()\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, scaler\n",
    "\n",
    "def undo_scaling(y_pred, x_test, scaler_obj, lstm=False):  \n",
    "    #reshape y_pred\n",
    "    y_pred = y_pred.reshape(y_pred.shape[0], 1, 1)\n",
    "    \n",
    "    if not lstm:\n",
    "        x_test = x_test.reshape(x_test.shape[0], 1, x_test.shape[1])\n",
    "    \n",
    "    #rebuild test set for inverse transform\n",
    "    pred_test_set = []\n",
    "    for index in range(0,len(y_pred)):\n",
    "        pred_test_set.append(np.concatenate([y_pred[index],x_test[index]],axis=1))\n",
    "        \n",
    "    #reshape pred_test_set\n",
    "    pred_test_set = np.array(pred_test_set)\n",
    "    pred_test_set = pred_test_set.reshape(pred_test_set.shape[0], pred_test_set.shape[2])\n",
    "    \n",
    "    #inverse transform\n",
    "    pred_test_set_inverted = scaler_obj.inverse_transform(pred_test_set)\n",
    "    \n",
    "    return pred_test_set_inverted\n",
    "\n",
    "# def load_original_df():\n",
    "#     #load in original dataframe without scaling applied\n",
    "#     original_df = pd.read_csv('../data/monthly_data_training.csv')\n",
    "#     original_df = original_df.drop([\"GL_account\", \"GL_account_description\"], axis = 1)\n",
    "# #     original_df.partition_ledger_year_month = pd.to_datetime(original_df.partition_ledger_year_month)\n",
    "# #     original_df.partition_ledger_year_month = original_df.partition_ledger_year_month.apply(lambda x: str(x)[:-12])\n",
    "#     print(original_df.partition_ledger_year_month[2])\n",
    "# #     original_df = original_df.groupby('date_general_ledger')['amount_usd'].sum().reset_index()\n",
    "# #     original_df.date_general_ledger = pd.to_datetime(original_df.date_general_ledger)\n",
    "#     print(original_df.shape)\n",
    "#     return original_df\n",
    "\n",
    "# def predict_df(unscaled_predictions, original_df):\n",
    "#     #create dataframe that shows the predicted sales\n",
    "#     result_list = []\n",
    "#     sales_dates = list(original_df[train_rows:].partition_ledger_year_month)\n",
    "#     print(len(sales_dates))\n",
    "#     act_sales = list(original_df[train_rows:].amount_usd)\n",
    "#     print(len(act_sales))\n",
    "#     print(len(unscaled_predictions))\n",
    "    \n",
    "#     for index in range(0,len(unscaled_predictions)-1):\n",
    "#         print(index)\n",
    "#         result_dict = {}\n",
    "#         result_dict['pred_value'] = int(unscaled_predictions[index][0] + act_sales[index])\n",
    "#         result_dict['amount_usd'] = sales_dates[index+1]\n",
    "#         result_list.append(result_dict)\n",
    "        \n",
    "#     df_result = pd.DataFrame(result_list)\n",
    "    \n",
    "#     return df_result\n",
    "\n",
    "# model_scores = {}\n",
    "\n",
    "# def get_scores(unscaled_df, original_df, model_name):\n",
    "#     print(unscaled_df.pred_value.shape)\n",
    "#     print(original_df.amount_usd[train_rows:-1].shape)\n",
    "#     rmse = np.sqrt(mean_squared_error(original_df.amount_usd[train_rows:-1], unscaled_df.pred_value))\n",
    "#     mae = mean_absolute_error(original_df.amount_usd[train_rows:-1], unscaled_df.pred_value)\n",
    "#     r2 = r2_score(original_df.amount_usd[train_rows:-1], unscaled_df.pred_value)\n",
    "#     model_scores[model_name] = [rmse, mae, r2]\n",
    "\n",
    "#     print(f\"RMSE: {rmse}\")\n",
    "#     print(f\"MAE: {mae}\")\n",
    "#     print(f\"R2 Score: {r2}\")\n",
    "    \n",
    "# def plot_results(results, original_df, model_name):\n",
    "\n",
    "#     fig, ax = plt.subplots(figsize=(15,5))\n",
    "#     sns.lineplot(original_df.date_general_ledger, original_df.amount_usd, data=original_df, ax=ax, \n",
    "#                  label='Original', color='mediumblue')\n",
    "#     sns.lineplot(results.date_general_ledger, results.pred_value, data=results, ax=ax, \n",
    "#                  label='Predicted', color='Red')\n",
    "    \n",
    "#     ax.set(xlabel = \"date_general_ledger\",\n",
    "#            ylabel = \"amount_usd\",\n",
    "#            title = f\"{model_name} Sales Forecasting Prediction\")\n",
    "    \n",
    "#     ax.legend()\n",
    "    \n",
    "#     sns.despine()\n",
    "    \n",
    "#     plt.savefig(f'../model_output/{model_name}_forecast.png')\n",
    "\n",
    "# LSTM Model Architecture:\n",
    "def lstm_model(train_data, test_data):\n",
    "    \n",
    "    X_train, y_train, X_test, y_test, scaler_object = scale_data(train_data, test_data)\n",
    "    \n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "   \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(4, batch_input_shape=(1, X_train.shape[1], X_train.shape[2]), \n",
    "                   stateful=True))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model.fit(X_train, y_train, epochs=1, batch_size=1, verbose=1, \n",
    "              shuffle=False)\n",
    "    predictions = model.predict(X_test,batch_size=1)\n",
    "    print(predictions.shape)\n",
    "\n",
    "    unscaled = undo_scaling(predictions, X_test, scaler_object, lstm=True)\n",
    "#     original_df = load_original_df()\n",
    "#     unscaled_df = predict_df(unscaled, original_df)\n",
    "#     get_scores(unscaled_df, original_df, 'LSTM')\n",
    "#     plot_results(unscaled_df, original_df, 'LSTM')\n",
    "    return model, unscaled\n",
    "\n",
    "\n",
    "# Loading the submission file with our ouput ledger year month to be predicted for. (i.e. 201907)\n",
    "\n",
    "test_df = pd.read_csv(\"submissionKey_2019_7_testfile.csv\")\n",
    "X_test = test_df.drop([\"account_id\"], axis=1)\n",
    "\n",
    "count = 0\n",
    "\n",
    "# Function for LSTM model with returning predicted values for \"201907\" partition_ledger_year_month\n",
    "# This function is called once for each account id as it is fired on groupby \"account_id\"\n",
    "def model(data, xvars, count):\n",
    "    train = data[xvars]\n",
    "    model, predictions = lstm_model(train.values, X_test.values)\n",
    "    count += 1\n",
    "    pickle.dump(model, open( \"model_scores_lstm.p\", \"wb\" ) )\n",
    "    return predictions[0][0]\n",
    "\n",
    "# Main driver code:\n",
    "# Function to apply model function on each group and passing columns \n",
    "# ['partition_ledger_year_month'], ['amount_usd'] as X and Y to fit the model:\n",
    "submission_df = model_df.groupby('account_id').apply(model, ['partition_ledger_year_month', 'amount_usd'], count).reset_index(name='amount_usd')\n",
    "\n",
    "print(submission_df.head())\n",
    "\n",
    "# Saving results to the submission csv:\n",
    "submission_df.to_csv(\"submissionKey_2019_7_LSTM_Try1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "54/54 [==============================] - 1s 11ms/step - loss: 181567438300615.1250\n",
      "Epoch 2/10\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 181567438115157.3438\n",
      "Epoch 3/10\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 181567438106415.4062\n",
      "Epoch 4/10\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 181567435767959.7188\n",
      "Epoch 5/10\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 181567435620598.5000\n",
      "Epoch 6/10\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 181567434943525.9375\n",
      "Epoch 7/10\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 181567433582554.0625\n",
      "Epoch 8/10\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 181567433432462.2188\n",
      "Epoch 9/10\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 181567433412968.2812\n",
      "Epoch 10/10\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 181567433415964.4375\n",
      "   account_id  amount_usd\n",
      "0    550385.0   -0.085591\n"
     ]
    }
   ],
   "source": [
    "# Loading the submission file with our ouput ledger year month to be predicted for. (i.e. 201907)\n",
    "test_df = pd.read_csv(\"submissionKey_2019_7_testfile.csv\")\n",
    "#X_test = test_df.drop([\"account_id\"], axis=1)\n",
    "\n",
    "# LSTM Model Architecture:\n",
    "def lstm_model(X_train, y_train, X_test):\n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, 1)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(4, batch_input_shape=(1, X_train.shape[1], X_train.shape[2]), stateful=True))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=1, verbose=1, \n",
    "              shuffle=False)\n",
    "    predictions = model.predict(X_test,batch_size=1)\n",
    "    return model,predictions\n",
    "\n",
    "\n",
    "# Function for LSTM model with returning predicted values for \"201907\" partition_ledger_year_month\n",
    "# This function is called once for each account id as it is fired on groupby \"account_id\"\n",
    "\n",
    "count = 0\n",
    "def model(data, xvars, yvar, count):\n",
    "    X_train = data[xvars]\n",
    "    Y_train = data[yvar]\n",
    "    model, predictions = lstm_model(X_train.values, Y_train.values, X_test.values[count])\n",
    "    count += 1\n",
    "    pickle.dump(model, open( \"model_scores_lstm.p\", \"wb\" ) )\n",
    "    return predictions[0][0]\n",
    "\n",
    "# Main driver code:\n",
    "# Function to apply model function on each group and passing columns \n",
    "# ['partition_ledger_year_month'], ['amount_usd'] as X and Y to fit the model:\n",
    "submission_df = model_df.groupby('account_id').apply(model, ['partition_ledger_year_month'], ['amount_usd'], count).reset_index(name='amount_usd')\n",
    "print(submission_df.head())\n",
    "submission_df.to_csv(\"submissionKey_2019_7_LSTM_Try2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Regression Models using transaction training data. ( Aim is to predict every day transaction for July 2019 (201907) and then sum it to get the monthly prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20150910\n",
      "(176718, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>account_id</th>\n",
       "      <th>partition_ledger_year_month</th>\n",
       "      <th>date_general_ledger</th>\n",
       "      <th>amount_usd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>751280</td>\n",
       "      <td>201807</td>\n",
       "      <td>20180717</td>\n",
       "      <td>4700449.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>751280</td>\n",
       "      <td>201808</td>\n",
       "      <td>20180830</td>\n",
       "      <td>5284250.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>751280</td>\n",
       "      <td>201510</td>\n",
       "      <td>20150910</td>\n",
       "      <td>3644851.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>751280</td>\n",
       "      <td>201902</td>\n",
       "      <td>20190213</td>\n",
       "      <td>4331.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>751280</td>\n",
       "      <td>201511</td>\n",
       "      <td>20151118</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   account_id  partition_ledger_year_month date_general_ledger  amount_usd\n",
       "0      751280                       201807            20180717  4700449.77\n",
       "1      751280                       201808            20180830  5284250.81\n",
       "2      751280                       201510            20150910  3644851.76\n",
       "3      751280                       201902            20190213     4331.25\n",
       "4      751280                       201511            20151118        0.00"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Common libraries and input data for both RF and XGBoost methods:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "# loading the cleaned training data from the EDA part:\n",
    "model_df = pd.read_csv('data_clean.csv')\n",
    "\n",
    "# Dropping columns \"company\", \"GL_account\" , \"GL_account_description\", \"document_type\" and \"address_number\"\n",
    "# as they are redundant and not required for the model to learn the amound_usd series:\n",
    "model_df = model_df.drop([\"Unnamed: 0\", \"company\", \"GL_account\", \"GL_account_description\", \"document_type\", \"address_number\"], axis = 1)\n",
    "\n",
    "# converting \"date_general_ledger\" to datetime:\n",
    "model_df.date_general_ledger = pd.to_datetime(model_df.date_general_ledger, dayfirst=True)\n",
    "# Getting proper date string which can be passed to model.fit method later where float values are required \n",
    "model_df.date_general_ledger = model_df.date_general_ledger.apply(lambda x: str(x)[:-15]+str(x)[5:-12]+str(x)[8:-9])\n",
    "print(model_df.date_general_ledger[2])\n",
    "print(model_df.shape)\n",
    "model_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Random Forest:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF Model 1: n_estimators=10, max_depth=3  (n_estimators means no. of trees and max depth of each tree )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   account_id    amount_usd\n",
      "0      550385 -4.386922e+07\n",
      "1      550406 -2.732936e+08\n",
      "2      661926  2.109200e+08\n",
      "3      693819 -3.450448e+03\n",
      "4      751280 -6.900958e+06\n"
     ]
    }
   ],
   "source": [
    "# Loading the submission file with our ouput ledger year month to be predicted for. (i.e. 201907)\n",
    "test_df = pd.read_csv(\"submissionKey_2019_7_testfile_datewise.csv\")\n",
    "\n",
    "# Dropping columns [\"account_id\",\"amount_usd\"] as we only require \"partition_ledger_year_month\" for X_test:\n",
    "X_test = test_df.drop([\"amount_usd\"], axis=1)\n",
    "\n",
    "# Function for Random Forest model with returning predicted values for \"201907\" partition_ledger_year_month\n",
    "# This function is called once for each account id as it is fired on groupby \"account_id\"\n",
    "# Function to apply model function on each group and passing columns \n",
    "# ['partition_ledger_year_month'], ['amount_usd'] as X and Y to fit the model:\n",
    "def model(df, model_object):\n",
    "    X = df[['date_general_ledger']].values\n",
    "    y = df[['amount_usd']].values\n",
    "    return model_object.fit(X, y.ravel()).predict(X_test)[0]\n",
    "\n",
    "# Function to apply model function on each group:\n",
    "# Grouping the training data by \"account_id\" and \"partition_ledger_year_month\"\n",
    "def group_predictions(df):\n",
    "    model_object = RandomForestRegressor(n_estimators=10, max_depth=3)\n",
    "    return df.groupby(['account_id', 'partition_ledger_year_month']).apply(model, model_object)\n",
    "\n",
    "# Main driver code\n",
    "submission_df = group_predictions(model_df).reset_index(name='amount_usd')\n",
    "\n",
    "# Group by sum of each account each month:\n",
    "submission_df = submission_df.groupby(['account_id'])['amount_usd'].apply(sum).reset_index(name='amount_usd')\n",
    "print(submission_df.head())\n",
    "\n",
    "# Saving results to the submission csv:\n",
    "submission_df.to_csv(\"submissionKey_2019_7_RandomForest_n_estimators_10_maxdepth_3_transactiondata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF Model 2: n_estimators=100, max_depth=4  (n_estimators means no. of trees and max depth of each tree )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   account_id    amount_usd\n",
      "0      550385  4.039934e+06\n",
      "1      550406 -2.148498e+08\n",
      "2      661926  2.191448e+08\n",
      "3      693819 -4.616872e+03\n",
      "4      751280 -2.991981e+07\n"
     ]
    }
   ],
   "source": [
    "# Loading the submission file with our ouput ledger year month to be predicted for. (i.e. 201907)\n",
    "test_df = pd.read_csv(\"submissionKey_2019_7_testfile_datewise.csv\")\n",
    "\n",
    "# Dropping columns [\"account_id\",\"amount_usd\"] as we only require \"partition_ledger_year_month\" for X_test:\n",
    "X_test = test_df.drop([\"amount_usd\"], axis=1)\n",
    "\n",
    "# Function for Random Forest model with returning predicted values for \"201907\" partition_ledger_year_month\n",
    "# This function is called once for each account id as it is fired on groupby \"account_id\"\n",
    "# Function to apply model function on each group and passing columns \n",
    "# ['partition_ledger_year_month'], ['amount_usd'] as X and Y to fit the model:\n",
    "def model(df, model_object):\n",
    "    X = df[['date_general_ledger']].values\n",
    "    y = df[['amount_usd']].values\n",
    "    return model_object.fit(X, y.ravel()).predict(X_test)[0]\n",
    "\n",
    "# Function to apply model function on each group:\n",
    "# Grouping the training data by \"account_id\" and \"partition_ledger_year_month\"\n",
    "def group_predictions(df):\n",
    "    model_object = RandomForestRegressor(n_estimators=20, max_depth=4)\n",
    "    return df.groupby(['account_id', 'partition_ledger_year_month']).apply(model, model_object)\n",
    "\n",
    "# Main driver code\n",
    "submission_df = group_predictions(model_df).reset_index(name='amount_usd')\n",
    "\n",
    "# Group by sum of each account each month:\n",
    "submission_df = submission_df.groupby(['account_id'])['amount_usd'].apply(sum).reset_index(name='amount_usd')\n",
    "print(submission_df.head())\n",
    "# Saving results to the submission csv:\n",
    "submission_df.to_csv(\"submissionKey_2019_7_RandomForest_n_estimators_100_maxdepth_4_transactiondata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. XGBoost:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model 1: n_estimators=10, learning_rate=0.3 (n_estimators means no. of trees and learning rate implies the rate at which the new values needs to be updated to the next training epoch/iteration )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   account_id    amount_usd\n",
      "0      550385 -2.019313e+07\n",
      "1      550406 -2.256978e+08\n",
      "2      661926  2.464053e+08\n",
      "3      693819 -1.220682e+04\n",
      "4      751280 -2.900487e+07\n"
     ]
    }
   ],
   "source": [
    "# Loading the submission file with our ouput ledger year month to be predicted for. (i.e. 201907)\n",
    "test_df = pd.read_csv(\"submissionKey_2019_7_testfile_datewise.csv\")\n",
    "\n",
    "# Dropping columns [\"account_id\",\"amount_usd\"] as we only require \"partition_ledger_year_month\" for X_test:\n",
    "X_test = test_df.drop([\"amount_usd\"], axis=1)\n",
    "\n",
    "# Function for Random Forest model with returning predicted values for \"201907\" partition_ledger_year_month\n",
    "# This function is called once for each account id as it is fired on groupby \"account_id\"\n",
    "# Function to apply model function on each group and passing columns \n",
    "# ['partition_ledger_year_month'], ['amount_usd'] as X and Y to fit the model:\n",
    "def model(df, model_object):\n",
    "    X = df[['date_general_ledger']].values\n",
    "    y = df[['amount_usd']].values\n",
    "    return model_object.fit(X, y.ravel()).predict(X_test.values)[0]\n",
    "\n",
    "# Function to apply model function on each group:\n",
    "# Grouping the training data by \"account_id\" and \"partition_ledger_year_month\"\n",
    "def group_predictions(df):\n",
    "    model_object = XGBRegressor(n_estimators=10, learning_rate=0.3, objective='reg:squarederror')\n",
    "    return df.groupby(['account_id', 'partition_ledger_year_month']).apply(model, model_object)\n",
    "\n",
    "# Main driver code\n",
    "submission_df = group_predictions(model_df).reset_index(name='amount_usd')\n",
    "\n",
    "# Group by sum of each account each month:\n",
    "submission_df = submission_df.groupby(['account_id'])['amount_usd'].apply(sum).reset_index(name='amount_usd')\n",
    "print(submission_df.head())\n",
    "\n",
    "# Saving results to the submission csv:\n",
    "submission_df.to_csv(\"submissionKey_2019_7_XGBoost_n_estimators_10_learningrate_0.3_transactiondata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model 2: n_estimators=100, learning_rate=0.2 (n_estimators means no. of trees and learning rate implies the rate at which the new values needs to be updated to the next training epoch/iteration )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   account_id    amount_usd\n",
      "0      550385 -2.085322e+07\n",
      "1      550406 -2.491265e+08\n",
      "2      661926  2.825468e+08\n",
      "3      693819 -1.520052e+04\n",
      "4      751280 -3.111376e+07\n"
     ]
    }
   ],
   "source": [
    "# Loading the submission file with our ouput ledger year month to be predicted for. (i.e. 201907)\n",
    "test_df = pd.read_csv(\"submissionKey_2019_7_testfile_datewise.csv\")\n",
    "\n",
    "# Dropping columns [\"account_id\",\"amount_usd\"] as we only require \"partition_ledger_year_month\" for X_test:\n",
    "X_test = test_df.drop([\"amount_usd\"], axis=1)\n",
    "\n",
    "# Function for Random Forest model with returning predicted values for \"201907\" partition_ledger_year_month\n",
    "# This function is called once for each account id as it is fired on groupby \"account_id\"\n",
    "# Function to apply model function on each group and passing columns \n",
    "# ['partition_ledger_year_month'], ['amount_usd'] as X and Y to fit the model:\n",
    "def model(df, model_object):\n",
    "    X = df[['date_general_ledger']].values\n",
    "    y = df[['amount_usd']].values\n",
    "    return model_object.fit(X, y.ravel()).predict(X_test.values)[0]\n",
    "\n",
    "# Function to apply model function on each group:\n",
    "# Grouping the training data by \"account_id\" and \"partition_ledger_year_month\"\n",
    "def group_predictions(df):\n",
    "    model_object = XGBRegressor(n_estimators=100, learning_rate=0.2, objective='reg:squarederror')\n",
    "    return df.groupby(['account_id', 'partition_ledger_year_month']).apply(model, model_object)\n",
    "\n",
    "# Main driver code\n",
    "submission_df = group_predictions(model_df).reset_index(name='amount_usd')\n",
    "\n",
    "# Group by sum of each account each month:\n",
    "submission_df = submission_df.groupby(['account_id'])['amount_usd'].apply(sum).reset_index(name='amount_usd')\n",
    "print(submission_df.head())\n",
    "\n",
    "# Saving results to the submission csv:\n",
    "submission_df.to_csv(\"submissionKey_2019_7_XGBoost_n_estimators_100_learningrate_0.2_transactiondata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
